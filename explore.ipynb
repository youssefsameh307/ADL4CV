{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mdm (Python 3.7.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/youssefabdelazim307/anaconda3/envs/mdm ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "data = torch.rand(64,200,3)\n",
    "\n",
    "my_layer = torch.nn.Linear(3,1)\n",
    "\n",
    "output = my_layer(data)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youssefhafez/anaconda3/envs/mdm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weights = torch.load(\"save/humanml_trans_enc_512/model000200000.pt\",map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ZeroConvBlock(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super(ZeroConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(input, output, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        nn.init.constant_(self.conv.weight, 0)\n",
    "        # nn.init.constant_(self.conv.bias, 0)\n",
    "        # nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        # You can replace this with any other suitable architecture\n",
    "        cachedResnet = models.resnet18(pretrained=True)\n",
    "        for param in cachedResnet.parameters():\n",
    "            param.requires_grad = False \n",
    "        cachedResnet.fc = nn.Identity()\n",
    "        self.cnn = cachedResnet\n",
    "        # self.cnn.fc = nn.Linear(self.cnn.fc.in_features, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad(): # We don't need this remove it\n",
    "            return self.cnn(x)\n",
    "\n",
    "\n",
    "class ModifiedTransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model ,nhead ,dim_feedforward ,dropout ,activation\n",
    "):\n",
    "        super(ModifiedTransformerEncoder, self).__init__()  \n",
    "        self.nheads = nhead\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.imageEmbedding = ImageEmbedding()\n",
    "        \n",
    "\n",
    "        self.inputConv = ZeroConvBlock(d_model, d_model)\n",
    "\n",
    "        # self.inputConv = ZeroConvBlock(image_condition.shape()[-1],self.d_model)\n",
    "\n",
    "        self.originalLayers = nn.ModuleList([nn.TransformerEncoderLayer(d_model=self.d_model,\n",
    "                                                              nhead=self.nheads,\n",
    "                                                              dim_feedforward=self.dim_feedforward,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation) for _ in range(num_layers)])\n",
    "        self.trainableLayers = nn.ModuleList([nn.TransformerEncoderLayer(d_model=self.d_model,\n",
    "                                                              nhead=self.nheads,\n",
    "                                                              dim_feedforward=self.dim_feedforward,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation) for _ in range(num_layers)])\n",
    "        \n",
    "        self.zeroConvLayers = nn.ModuleList([ZeroConvBlock(self.d_model, self.d_model) for _ in range(num_layers)])\n",
    "\n",
    "        # Check if GPU is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Move the model to the chosen device\n",
    "        self.to(self.device)  \n",
    "\n",
    "        # Set requires_grad to False for the parameters of the original layers\n",
    "        for layer in self.originalLayers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def loadCondition(self, condition):\n",
    "        self.image_condition = condition.to(self.device)\n",
    "        # self.image_condition = condition\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial processing of the condition\n",
    "        x = x.to(self.device)\n",
    "        condition_embedding = self.imageEmbedding(self.image_condition)\n",
    "        \n",
    "        condition_embedding = condition_embedding.view(1, -1).repeat(x.size(0), 1).view(x.size(0), x.size(1), -1)\n",
    "        # Apply the ZeroConvBlock\n",
    "        condition_embedding = self.inputConv(condition_embedding.permute(1, 2, 0)).permute(2, 0, 1)\n",
    "\n",
    "        trainableOutput = x + condition_embedding\n",
    "        \n",
    "        originalOutput = x\n",
    "        \n",
    "        for i in range(len(self.trainableLayers)):\n",
    "            originalLayer = self.originalLayers[i]\n",
    "            trainableLayer = self.trainableLayers[i]\n",
    "            convBlock = self.zeroConvLayers[i]\n",
    "\n",
    "            originalIntermediate = originalLayer(originalOutput)\n",
    "            \n",
    "            trainableOutput = trainableLayer(trainableOutput)\n",
    "            \n",
    "            convOutput = convBlock(trainableOutput.permute(1, 2, 0)).permute(2, 0, 1)\n",
    "            \n",
    "            originalOutput = originalIntermediate + convOutput\n",
    "        \n",
    "        return originalOutput\n",
    "    \n",
    "    def load_original_weights(self, state_dict):\n",
    "        \n",
    "        # Iterate over each layer in originalLayers and load the corresponding weights\n",
    "        for i, layer in enumerate(self.originalLayers):\n",
    "            # Construct the keys for the encoder layer's parameters\n",
    "            layer_state_dict = {k.replace(f'seqTransEncoder.layers.{i}.', ''): v \n",
    "                                for k, v in state_dict.items() if f'seqTransEncoder.layers.{i}.' in k}\n",
    "            layer.load_state_dict(layer_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlformer = ModifiedTransformerEncoder(num_layers=8, d_model=512, nhead=4, dim_feedforward=1024, dropout=0.1, activation='gelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlformer.load_original_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = torch.randn(64, 3, 480, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlformer.loadCondition(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(197, 64, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = controlformer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Forward pass to create the computational graph\n",
    "# output = model(dummy_input,1)\n",
    "\n",
    "# Visualize the model\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('model_visualization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "# model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(2, 22,263,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.input_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = layer(dummy_input)\n",
    "dot = make_dot(output, params=dict(layer.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('model_visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Assuming your input shape is (1, 3, 32, 32) for an image model\n",
    "dummy_input = torch.randn(1, 263)\n",
    "dot = make_dot( (dummy_input))\n",
    "dot.render(\"model_architecture.gv\", view=False)  # Save as a Graphviz file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
